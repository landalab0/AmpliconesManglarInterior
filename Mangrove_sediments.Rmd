---
title: "Mangrove sediments"
author: "DianaOaxaca"
date: "2024-02-27"
output:
  html_document:                   
    collapsed: true               
    code_folding: show             
    toc: true                     
    toc_depth: 4                   
    toc_float: true                 
    smooth_scroll: true            
    highlight: tango               
    df_print: paged                
    number_sections: true          
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**16S Amplicon analysis of Interior Mangrove Project**

Intro to project

# Prepare data

Get fastq files of sediments

```
mkdir data && cd data
bash get_fastq_files.sh
```

```
fastqc -o results/01.fastqc/ data*.gz
multiqc results/01.fastqc/*.zip -o results/01.fastqc/multiqc
```

```
nohup bash src/02.cutadapt.sh > outs/02.cutadapt.nohup &
```

```
mkdir -p results/02.cutadapt/fastqc
fastqc -t 80 -o results/02.cutadapt/fastqc results/02.cutadapt/*.gz
multiqc results/02.cutadapt/fastqc/*.zip -o results/02.cutadapt/multiqc
```

# Get ASVs


```{r}
library(dada2)
```
 
## Check quality 


```
#Load trim fastq files and list fastq_path content
fastq_path <- "/axolote/diana/manglares/results/02.cutadapt"

#Sort file names
Fs <- sort(list.files(fastq_path, pattern="_1.fastq"))
Rs <- sort(list.files(fastq_path, pattern="_2.fastq"))

# Extract sample names
sampleNames <- sapply(strsplit(Fs, "_1"), `[`, 1)
sampleNames
# Add complete path to remove ambiguities errors
Fs <- file.path(fastq_path, Fs)
Rs <- file.path(fastq_path, Rs)

# Quality check plot with only the first fastq file
QCplot_1 <- plotQualityProfile(c(rbind(Fs[1],Rs[1])))
pdf("results/plots/02.dada/01.QCplot_1.pdf")
QCplot_1
dev.off()

```


```{r echo=FALSE, out.width='70%', out.height='70%', fig.align='center'}
knitr::include_graphics("results/plots/02.dada/01.QCplot_1.pdf")

```


## Filter

### explore trunclen versions

```
# Create directory for clean reads
filt_path <- file.path("results/03.Dada2" , "filter_reads") 
if(!file_test("-d", filt_path)) 
  dir.create(filt_path)
filtFs <- file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
filtRs <- file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))

# Quality control
# V1 
out1 <- filterAndTrim(Fs, filtFs, Rs, filtRs,
                      truncLen=c(250,200),
                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) 

#V2 extra permissive
out2 <- filterAndTrim(Fs, filtFs, Rs, filtRs,
                      truncLen=c(0,0),
                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) 


#V3
out3 <- filterAndTrim(Fs, filtFs, Rs, filtRs,
                      truncLen=c(280,210),
                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) 

##v4
out4 <- filterAndTrim(Fs, filtFs, Rs, filtRs,
                      truncLen=c(0,200),
                      maxN=0, maxEE=c(5,5), truncQ=2, rm.phix=TRUE,
                      compress=TRUE, multithread=TRUE) 
```

### Compare versions

```
#convert double as DataFrame
v1 <- as.data.frame(out1)
v2 <- as.data.frame(out2)
v3 <- as.data.frame(out3)
v4 <- as.data.frame(out4)

# percentage function
calculate_percentage <- function(df, group_name) {
  df$percentage <- (df$reads.out / df$reads.in) * 100
  df$version <- group_name
  return(df)
}

# Get percentage to each version
out1_with_percentage <- calculate_percentage(v1, 'out1')
out2_with_percentage <- calculate_percentage(v2, 'out2')
out3_with_percentage <- calculate_percentage(v3, 'out3')
out4_with_percentage <- calculate_percentage(v4, 'out4')

# Combine
combined_data <- rbind(out1_with_percentage, out2_with_percentage, out3_with_percentage, out4_with_percentage)

# Compare in a Boxplot
library(ggplot2)

filter_versions <- ggplot(combined_data, aes(x = version, y = percentage, fill = version)) +
          geom_boxplot() +
          labs(x = "Filter version", y = "Percentage of reads after filter") +
          theme_bw() +
          scale_fill_brewer(palette = "Set2")

pdf("results/plots/02.dada/02.Filter_versions.pdf")
filter_versions
dev.off()

#Save info of final version
write.table(out1, file="results/03.Dada2/Dada_clean.tsv", quote=F, sep="\t",col.names=NA) # Table with the totals before and after cleaning

```

```{r echo=FALSE, out.width='70%', out.height='70%', fig.align='center'}
knitr::include_graphics("results/plots/02.dada/02.Filter_versions.pdf")

```
## Error model

```
#De-replicate to reduce redundance 

derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)

# Add names to de-rep object
names(derepFs) <- sampleNames
names(derepRs) <- sampleNames

#Generate error model 
errF <- learnErrors(derepFs, multithread=TRUE, verbose=TRUE)
errR <- learnErrors(derepRs, multithread=TRUE, verbose=TRUE)
```
## ASVs inference

pseudo pooling explanation [here](https://benjjneb.github.io/dada2/pseudo.html)

```
dadaFs <- dada(derepFs, err=errF, multithread=TRUE, pool = "pseudo", verbose=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE, pool = "pseudo", verbose = TRUE)
```
### Merge and remove chimeras

```
# Merge pairs
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, minOverlap = 8, verbose=TRUE)

# Create ASVs table 
seqtabAll <- makeSequenceTable(mergers)
table(nchar(getSequences(seqtabAll)))

# Remove chimeras
seqtab_nochim <- removeBimeraDenovo(seqtabAll, method="consensus", multithread=TRUE, verbose=TRUE)
```

### Get feature table and representative sequences

```
# create a new table with each ASV number and its representative sequence
PE.table_tsv_output <- seqtab_nochim
PE.table_tsv_output[PE.table_tsv_output==1]=0 # Don't consider those values that have a single observation per sample, make them 0 (sample singletons)
PE.table_tsv_output <- PE.table_tsv_output[,colSums(PE.table_tsv_output)>1] # filter singleton ASVs across the table

# Export sequences as in fasta format
uniquesToFasta(PE.table_tsv_output, fout="results/03.Dada2/ASVs.fasta", ids=paste("ASV_",1:ncol(PE.table_tsv_output), sep=""))
nochim=PE.table_tsv_output
write.table(cbind("ASVs"=1:nrow(t(PE.table_tsv_output)),"rep_seq"=rownames(t(PE.table_tsv_output))), file="results/03.Dada2/ASV_to_seqs-nochim.tsv", quote=F, sep="\t",row.names=FALSE)

# replace the rep_seq with an incremental ASV number
PE.table_tsv_output <- t(PE.table_tsv_output)
rownames(PE.table_tsv_output) <- paste0("ASV_",1:nrow(PE.table_tsv_output))

# and save ASV table
write.table(PE.table_tsv_output, file="results/03.Dada2/ASV_to_seqs-nochim.tsv", quote=F, sep="\t",col.names=NA)
```

### Get track summary per step

```
# By using this, we can create a function to automate this for all samples in a set:
getN <- function(x) sum(getUniques(x)) # Where getUniques gets non-repeated sequences from a dada2 object or merger object (joined reads)
track <- cbind(out1, sapply(derepFs, getN), sapply(dadaFs, getN), sapply(dadaRs, getN), rowSums(seqtabAll), rowSums(nochim))
colnames(track) <- c("Raw", "Qual_filter", "Derep", "ASVs R1", "ASVs R2", "Merged", "nonchim")
rownames(track) <- sampleNames
write.table(track, "results/03.Dada2/Seqs_lost_in_ASVs_processing.tsv", col.names=NA, sep="\t")


# Create a quick assesment of sequences lost throughout the process
pdf("results/plots/02.dada/03.sequences_throughout_ASV_process.pdf")

# And same thing for the percentage remaining
matplot(t(track[,-5]/track[,1]*100),type='l',xaxt='n', main="Sequences remaining after each step  - R1 (%)", xlab="Step", ylab=" Percentage of Sequences remaining")
axis(1,at=1:ncol(track[,-5]),labels=colnames(track[,-5]))
# R2
matplot(t(track[,-4]/track[,1]*100),type='l',xaxt='n', main="Sequences remaining after each step  - R2 (%)", xlab="Step", ylab=" Percentage of Sequences remaining")
axis(1,at=1:ncol(track[,-4]),labels=colnames(track[,-4]))

dev.off()

##Add final table
track2 <- data.frame(track)
track2$percentage_used <-(track2$nonchim / track2$Raw) * 100
track2
write.table(track2, "results/03.Dada2/Seqs_lost_in_ASVs_processing_percentage.tsv", col.names=NA, sep="\t")

# Save work so far
save.image(file = "Dada2.RData") 

```

```{r echo=FALSE, out.width='100%', out.height='100%', fig.align='center'}
knitr::include_graphics("results/plots/02.dada/03.sequences_throughout_ASV_process.pdf")
```

# Get taxonomy and phylogeny

## Train database

Train the Silva database taking V3-V4 amplicon to increase the taxonomic assignment resolution.

```
mkdir -p data/dbs

#get the database and check its integrity

wget https://data.qiime2.org/2023.5/common/silva-138-99-seqs.qza

md5sum data/dbs/silva-138-99-seqs.qza 
de8886bb2c059b1e8752255d271f3010  data/dbs/silva-138-99-seqs.qza

wget https://data.qiime2.org/2023.5/common/silva-138-99-tax.qza

md5sum data/dbs/silva-138-99-tax.qza 
f12d5b78bf4b1519721fe52803581c3d  data/dbs/silva-138-99-tax.qza
```

```
conda activate qiime2-2023.5
#extract specific fragments
qiime feature-classifier extract-reads \
--i-sequences data/dbs/silva-138-99-seqs.qza \
--p-f-primer CCTACGGGNGGCWGCAG --p-r-primer GACTACHVGGGTATCTAATCC \
--p-min-length 250 --p-max-length 450 \
--o-reads data/dbs/silva-138-99-seqs-extracted.qza --p-n-jobs 40
```

```
#train the database
qiime feature-classifier fit-classifier-naive-bayes \
--i-reference-reads data/dbs/silva-138-99-seqs-extracted.qza \
--i-reference-taxonomy data/dbs/silva-138-99-tax.qza \
--o-classifier data/dbs/classifier_silva_138_trained.qza
```

## Import data to QIIME2

```
mkdir -p results/04.qiime

#import rep seqs
qiime tools import --input-path results/03.Dada2/ASVs.fasta --type 'FeatureData[Sequence]' --output-path results/04.qiime/ASV_rep_seq.qza

# append missing header to the table for import
cat <(echo -n "#OTU Table") results/03.Dada2/ASV_to_seqs-nochim.tsv > temp.txt

# convert to biom
biom convert -i temp.txt -o temp.biom --table-type="OTU table" --to-hdf5

# and create table-type qza
qiime tools import --input-path temp.biom --type 'FeatureTable[Frequency]' --input-format BIOMV210Format --output-path results/04.qiime/ASV_table.qza
```

## Taxonomy assignment

```
#taxonomy assignment
qiime feature-classifier classify-sklearn \
  --i-classifier data/dbs/classifier_silva_138_trained.qza \
  --i-reads results/04.qiime/ASV_rep_seq.qza \
  --o-classification results/04.qiime/taxonomy.qza --p-n-jobs 40

#get visualization
qiime metadata tabulate \
  --m-input-file results/04.qiime/taxonomy.qza \
  --o-visualization results/04.qiime/taxonomy.qzv

#get visual fasta to compare the taxonomic assignments with the top BLASTn hits for certain ASVs  
qiime feature-table tabulate-seqs \
--i-data results/04.qiime/ASV_rep_seq.qza \
--o-visualization results/04.qiime/ASV_rep_seq.qzv
```

### Filters

```
#Summary of the qza table imported from R
qiime feature-table summarize \
--i-table results/04.qiime/ASV_table.qza \
--o-visualization results/04.qiime/ASV_table_summary.qzv
```

```
qiime taxa filter-table --i-table results/04.qiime/ASV_table.qza --i-taxonomy results/04.qiime/taxonomy.qza --p-exclude Archaea,Eukarya,mitochondria,chloroplast --p-include p__ --o-filtered-table results/04.qiime/ASV_table_filter_aemc.qza

qiime feature-table summarize --i-table results/04.qiime/ASV_table_filter_aemc.qza --o-visualization results/04.qiime/ASV_table_summaryfilter_aemc.qzv

```

Here I removed all ASVs with a frequency of less than 0.1% of the mean sample depth. This cut-off excludes ASVs that are likely due to MiSeq bleed-through between runs (reported by Illumina to be 0.1% of reads). To calculate this cut-off I identified the mean sample depth, multiplied it by 0.001, and rounded to the nearest integer. This step are describe in [this paper](https://journals.asm.org/doi/pdf/10.1128/msystems.00127-16)


```
qiime feature-table filter-features --i-table  results/04.qiime/ASV_table_filter_aemc.qza --p-min-samples 1 --p-min-frequency 94 --o-filtered-table results/04.qiime/ASV_table_filter_aemc_freq94_1minsamp.qza

qiime feature-table summarize --i-table results/04.qiime/ASV_table_filter_aemc_freq94_1minsamp.qza --o-visualization results/04.qiime/ASV_table_filter_aemc_freq94_1minsamp.qzv
```

```
#remove in fasta sequences
qiime feature-table filter-seqs  --i-table results/04.qiime/ASV_table_filter_aemc_freq94_1minsamp.qza --i-data results/04.qiime/ASV_rep_seq.qza --o-filtered-data results/04.qiime/ASV_rep_seq_filters.qza
```

Exports to text format

```
#taxonomy
qiime tools export --input-path results/04.qiime/taxonomy.qza --output-path results/04.qiime/exports/taxonomy

#feature table
qiime tools export --input-path results/04.qiime/ASV_table_filter_aemc_freq94_1minsamp.qza --output-path results/04.qiime/exports/ASV_table

#reformat taxonomy tsv
sed -i -e '1 s/Feature/#Feature/' -e '1 s/Taxon/taxonomy/' results/04.qiime/exports/taxonomy/taxonomy.tsv

#Add taxonomy to feature table
biom add-metadata -i results/04.qiime/exports/ASV_table/feature-table.biom -o results/04.qiime/exports/feature-table_tax.biom --observation-metadata-fp results/04.qiime/exports/taxonomy/taxonomy.tsv --sc-separated results/04.qiime/exports/taxonomy/taxonomy.tsv

#Convert to tsv from biom format
biom convert -i results/04.qiime/exports/feature-table_tax.biom -o results/04.qiime/exports/feature-table_tax.tsv --to-tsv --header-key taxonomy
```



## Phylogeny

```
qiime phylogeny align-to-tree-mafft-fasttree \
--p-n-threads auto --i-sequences results/04.qiime/ASV_rep_seq_filters.qza \
--o-alignment results/04.qiime/align2.qza \
--o-masked-alignment results/04.qiime/masked-align-fasttree2.qza \
--o-tree results/04.qiime/unrooted-tree-fasttree2.qza \
--o-rooted-tree results/04.qiime/rooted-tree-fasttree2.qza --verbose
```

# Diversity

## Explore data

load packages

```{r}
load("Postprocess.RData")
```

```{r include=FALSE}
#Define package vectors
cran_packages <- c("knitr", "qtl", "bookdown", "magrittr", "plyr", "ggplot2",
                   "grid","gridExtra", "tidyverse", "devtools", "dplyr",
                   "pheatmap", "xtable",
                   "kableExtra", "remotes", "Rtsne", "vegan", "RColorBrewer",
                   "PoiClaClu",
                   "gtools", "gplots", "reshape2", "MASS", "usethis",
                   "indicspecies", "Polychrome")

bioc_packages <- c("airway", "phyloseq", "dada2", "DECIPHER", "phangorn",
                   "ggpubr","DESeq2",
                   "genefilter", "philr", "GenomeInfoDb", "microbiome",
                   "metagenomeSeq", "mia", "curatedMetagenomicData",
                   "ANCOMBC","microbiomeMarker")

git_packages <- c("btools", "fantaxtic", "ampvis2", "tsnemicrobiota",
                  "qiime2R", "ranacapa")
                  
#Load libraries
sapply(c(cran_packages, bioc_packages, git_packages), require, character.only = TRUE)


```

### Import data to R from qiime and check prevalence to filter data 

```

# 01. Load data
physeq_qiime2 <- qza_to_phyloseq(
  features = "results/04.qiime/ASV_table_filter_aemc_freq94_1minsamp.qza",
  tree = "results/04.qiime/rooted-tree-fasttree2.qza",
  taxonomy = "results/04.qiime/taxonomy.qza",
  metadata = "data/Metadatos_Rio_San_Pedro_actualizados.tsv")

```

```{r}
physeq_qiime2
```


```
# 02. Explore prevalence
## 02.1 Get prevalence
prevdf = apply(X = otu_table(physeq_qiime2),
               MARGIN = ifelse(taxa_are_rows(physeq_qiime2), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})
## 02.2 Add taxonomy
prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(physeq_qiime2),
                    tax_table(physeq_qiime2))
## 02.3 Check prevalence at Phylum level
dfprev <- plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})

## 02.4 Get genus prevalence plot
prevalence_genus = subset(prevdf, Genus %in% get_taxa_unique(physeq_qiime2, "Genus"))
prev_genus <- ggplot(prevalence_genus, aes(TotalAbundance, 
  Prevalence /nsamples(physeq_qiime2),color=Genus)) +
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  xlab("Total Abundance") + ylab("Prevalence") +
  facet_wrap(~Phylum) + theme(legend.position="none")

#save prevalence plot
pdf("results/plots/03.diversity/01.Prevalence_Phylum_and_genus.pdf")
prev_genus
dev.off()
```

```{r echo=FALSE, out.width='100%', out.height='100%', fig.align='center'}
knitr::include_graphics("results/plots/03.diversity/01.Prevalence_Phylum_and_genus.pdf")
```

### Check sample effort

```
## 3.1 FASTER rarefaction curves with Vegan
mat <- as(t(otu_table(physeq_qiime2)), "matrix")
raremax <- min(rowSums(mat))
pdf("results/plots/03.diversity/02.Rarefaction_curves_vegan.pdf")
vegan_rarefaction_curves <- system.time(rarecurve(mat, step = 100, sample = raremax, 
                                                  col = "blue", label = FALSE))
dev.off()
```

```
#Acumulation curves
#library(ranacapa)
## 03.2 Get accumulation curves
acumulation_curves <- ggrare(physeq_qiime2, step = 100, color = "Location", label = "Sample")

#custom plot
acumulation_curves_plot <- acumulation_curves + facet_wrap(~Location) +
  labs(title="Accumulative curves") + theme_bw()

#save plot
pdf("results/plots/03.diversity/03.Rarefaction_curves_ranacapa.pdf")
acumulation_curves_plot
dev.off()
```

```{ number_samples}
library(ggplot2)

freq_samples <- plot_frequencies(sample_data(physeq_qiime2), "Site", "Depth") + theme_bw()
pdf("results/plots/03.diversity/Number_of_Samples.pdf")
freq_samples <- ggplot(freq_samples$data, aes(x = Groups, y = n, fill = Factor)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Number of samples of Site and Depth") +
  theme_bw() + scale_color_brewer(palette = "Set2") + scale_fill_brewer(palette = "Set2")
dev.off()
freq_samples
```

```{r}
freq_samples
```

### Beta diversity

#### Bray-curtis

```
#### ANOSIM
#extract metadata
metadata <- data.frame(phyloseq::sample_data(physeq_qiime2),
           check.names = FALSE)
#get significant difference between location with anosim
anosim_location <- anosim(x= as.data.frame(t(otu_table(physeq_qiime2))),
       grouping = metadata$Location,
       permutations = 9999, distance = "bray")
#get values
anosim_significance <- anosim_location$signif
anosim_statistic  <- anosim_location$statistic
```

```{r}
anosim_location
```


```
# Bray NMDS 
nmds_bray <- ordinate(physeq_qiime2, method = "NMDS", distance = "bray")
# Get stress value
var_stress_nmds_bray <- round(nmds_bray$stress, 5)

#checks that the fit is good with shepard plot
stressplot(nmds_bray)

nmds_bray_plot <- plot_ordination(physeq_qiime2, nmds_bray, label = "Sample",
                           color = "Location", shape = "Location") + theme_bw() + 
  labs(col = "Location") + labs(title="NMDS, Bray-Curtis distance\nANOSIM Location: R=0.3246, p=0.0001, Permutations=9999") +
  geom_point(size=3) + theme_bw() 

nmds_bray_plot <- nmds_bray_plot +
  annotate("text", x = Inf, y = -Inf, label = paste("Stress:", var_stress_nmds_bray),
           hjust = 2.1, vjust = -1.9, size = 4)
nmds_bray_plot

pdf("results/plots/03.diversity/04.NMDS_Shepard_Bray_Fit.pdf")
stressplot(nmds_bray)
dev.off()

pdf("results/plots/03.diversity/05.NMDS_Bray_Location.pdf")
nmds_bray_plot
dev.off()

```

```{r}
print(stressplot(nmds_bray))
```

```{r}
print(nmds_bray_plot)
```

#### Weighted Unifraq

```
#Weigthed UniFrac take relative abundance and it is less sensitive to sample size 

nmds_wunifrac <- ordinate(physeq_qiime2, method = "NMDS", distance = "wunifrac")
# stress variable
var_stress_nmds_wu <- round(nmds_wunifrac$stress, 5)
var_stress_nmds_wu

stressplot(nmds_wunifrac)# checks that the fit is good
#nmds_wunifrac$points
```

```
# Weigthed UniFrac NMDS
nmds_wu <- plot_ordination(physeq_qiime2, nmds_wunifrac, label = "Sample",
                           color = "Location", shape = "Location") + theme_bw() + 
  labs(col = "Location") + 
  labs(title="NMDS, Weighted UniFrac distance") +
  geom_point(size=3) + theme_bw() 

nmds_wu <- nmds_wu +
  annotate("text", x = Inf, y = -Inf, label = paste("Stress:", var_stress_nmds_wu),
           hjust = 1.1, vjust = -1.1, size = 4)
nmds_wu

pdf("results/plots/03.diversity/06.NMDS_Shepard_WUniFrac_Fit.pdf")
stressplot(nmds_wunifrac)
dev.off()

pdf("results/plots/03.diversity/07.NMDS_WUniFrac_Location.pdf")
nmds_wu
dev.off()
```

```{r}
print(nmds_wu)
```

#### Bray-Curtis with new metadata

```
#extract metadata
metadata <- data.frame(phyloseq::sample_data(physeq_qiime2),
           check.names = FALSE)
```

##### Depth
```

#get significant difference between Depth with anosim
anosim_depth <- anosim(x= as.data.frame(t(otu_table(physeq_qiime2))),
       grouping = metadata$Depth,
       permutations = 9999, distance = "bray")
```

```{r}
anosim_depth
```

```
nmds_bray_depth_plot <- plot_ordination(physeq_qiime2, nmds_bray, label = "Sample",
                           color = "Depth", shape = "Location") + theme_bw() + 
  labs(col = "Depth") + labs(title="NMDS, Bray-Curtis distance\nANOSIM Depth: R=0.1356, p=0.0045, Permutations=9999") +
  geom_point(size=3) + theme_bw() 

nmds_bray_depth_plot <- nmds_bray_depth_plot +
  annotate("text", x = Inf, y = -Inf, label = paste("Stress:", var_stress_nmds_bray),
           hjust = 2.1, vjust = -1.9, size = 4)
```

```{r}
nmds_bray_depth_plot
```
##### Site

```

#get significant difference between Depth with anosim
anosim_site <- anosim(x= as.data.frame(t(otu_table(physeq_qiime2))),
       grouping = metadata$Site,
       permutations = 9999, distance = "bray")
```

```{r}
anosim_site
```

```
nmds_bray_site_plot <- plot_ordination(physeq_qiime2, nmds_bray, label = "Sample",
                           color = "Site", shape = "Depth") + theme_bw() + 
  labs(col = "site") + labs(title="NMDS, Bray-Curtis distance\nANOSIM Site: R=0.4105, p=0.0001, Permutations=9999") +
  geom_point(size=3) + theme_bw() 

nmds_bray_site_plot <- nmds_bray_site_plot +
  annotate("text", x = Inf, y = -Inf, label = paste("Stress:", var_stress_nmds_bray),
           hjust = 2.1, vjust = -1.9, size = 4)
```

```{r}
nmds_bray_site_plot
```



### Alpha diversity

```
#Get diversity index
diversity_all_index <- microbiome::alpha(physeq_qiime2, index = "all")
#save table
write.table(diversity_all_index, "results/05.postprocess/all_alpha_index.tsv", col.names=NA, sep="\t")
```

#### obs and shannon

```{r}


### Get simple alpha diversity plots by location, practices, and both

alpha_depth_plot <- plot_richness(physeq_qiime2, color = "Depth", x = "Depth", measures = c("Observed", "Shannon")) +
  geom_boxplot(aes(fill = Depth), alpha=.7) + scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") + theme_bw()



alpha_location_plot <- plot_richness(physeq_qiime2, color = "Location", x = "Location", measures = c("Observed", "Shannon")) +
  geom_boxplot(aes(fill = Location), alpha=.7) + scale_color_brewer(palette = "Set2") +
  scale_fill_brewer(palette = "Set2") +theme_bw() + theme(axis.text.x  = element_text(angle = 10))


alpha_site_plot <- plot_richness(physeq_qiime2, color = "Site", x = "Site", measures = c("Observed", "Shannon")) +
  geom_boxplot(aes(fill = Site), alpha=.7) + scale_color_brewer(palette = "Set1") +
  scale_fill_brewer(palette = "Set1") +theme_bw() + theme(axis.text.x  = element_text(angle = 10))


alpha_site_and_depth_plot <-plot_richness(physeq_qiime2, color = "Site", x = "Site", measures = c("Observed", "Shannon")) +
  geom_boxplot(aes(fill = Depth), alpha=.7) + scale_color_brewer(palette = "Dark2") +
  scale_fill_brewer(palette = "Dark2") + theme_bw() + theme(axis.text.x  = element_text(angle = 10))

```

```{r}
alpha_depth_plot
```

```{r}
alpha_location_plot
```

```{r}
alpha_site_plot
```


```{r}
alpha_site_and_depth_plot
```


#### Get Observed, shannon, Faith and Pielou

```
# Estimate faith diversity
alpha_pd <- estimate_pd(physeq_qiime2)

#Calculating Faiths PD-index...
alpha_pd

### get specific index diversity with metadata 
physeq_with_metadata <- meta(physeq_qiime2)

#Add specific diversity index to metadata
physeq_with_metadata$Shannon <- diversity_all_index$diversity_shannon
physeq_with_metadata$Observed <- diversity_all_index$observed
physeq_with_metadata$Pielou <- diversity_all_index$evenness_pielou
physeq_with_metadata$Faith <- alpha_pd$PD #faith

#save table
write.table(as.data.frame(physeq_with_metadata), "results/05.postprocess/metadata_with_specific_alpha_index_table.tsv", sep="\t")

```

```{r}
meta_div <- as.data.frame(physeq_with_metadata)
sites <- unique(meta_div$Site)
sites_par <- combn(seq_along(sites), 2, simplify = FALSE, FUN = function(i)sites[i])
# Imprimimos en pantalla el resultado
print(sites_par)

#grafico de violin con Shannon
sites_shannon <- ggboxplot(physeq_with_metadata, x = "Site", y = "Shannon",
                           fill = "Site") + scale_color_brewer(palette  = "Set1")+ 
                scale_fill_brewer(palette  = "Set1")+ theme_bw()

sites_shannon
```


```{r}

sites_shannon_sig <- sites_shannon + stat_compare_means(comparisons = sites_par)
sites_shannon_sig

```









